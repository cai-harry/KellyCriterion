{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train example\n",
    "\n",
    "Use this notebook to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Notebook setup\n",
    "\n",
    "import os\n",
    "REPO_ROOT = \"D:\\\\play\\\\kelly-criterion\"\n",
    "os.chdir(REPO_ROOT)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports and function definitions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from src.environment import Environment\n",
    "from src.agent import Agent\n",
    "\n",
    "def run_training(train_from_fresh, num_episodes, epsilons, exploring_start=False):\n",
    "    env = Environment()\n",
    "\n",
    "    if train_from_fresh:\n",
    "        agent = Agent(env)\n",
    "\n",
    "        total_rewards = agent.train(env, num_episodes=num_episodes, epsilons_each_episode=epsilons,\n",
    "                                    exploring_start=exploring_start, plot_training_rewards=True,\n",
    "                                   use_tensorboard=True)\n",
    "\n",
    "        agent.plot_N_values()\n",
    "\n",
    "    else:\n",
    "        agent = Agent(Q_values=np.loadtxt(\"Q.csv\"))\n",
    "\n",
    "    agent.plot_Q_values()\n",
    "\n",
    "    agent.plot_policy(\n",
    "        optimal_policy=[min(250 - s, 0.2 * s) for s in range(1, 250)]\n",
    "    )\n",
    "\n",
    "\n",
    "def epsilon_picket_fence(num_episodes: int, explore_ratio: float, repeats: int):\n",
    "    repeating_phase_length = num_episodes // repeats\n",
    "    single_explore_phase_length = int(repeating_phase_length * explore_ratio)\n",
    "    single_exploit_phase_length = repeating_phase_length - single_explore_phase_length\n",
    "\n",
    "    single_repeating_phase = np.concatenate([\n",
    "        np.ones(single_explore_phase_length),\n",
    "        np.zeros(single_exploit_phase_length)\n",
    "    ])\n",
    "\n",
    "    return np.tile(single_repeating_phase, repeats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Run the training\n",
    "\n",
    "DESIRED_TRAIN_NUM_SECONDS = 2 * 60 * 60  # 2 hours\n",
    "APPROX_EPISODES_PER_SECOND = 1100\n",
    "\n",
    "train_num_episodes = APPROX_EPISODES_PER_SECOND * DESIRED_TRAIN_NUM_SECONDS\n",
    "epsilon_fn = epsilon_picket_fence(num_episodes=train_num_episodes,\n",
    "                                  explore_ratio=0.9,\n",
    "                                  repeats=5)\n",
    "\n",
    "run_training(train_from_fresh=True,\n",
    "           num_episodes=train_num_episodes,\n",
    "           epsilons=epsilon_fn,\n",
    "           exploring_start=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
